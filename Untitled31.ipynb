{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YOxzY5jLwrk",
        "outputId": "fa30a90d-ac85-4ea1-8e0f-01da10061e40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Google Trends Keyword Analyzer\n",
            "==================================================\n",
            "Enter your SerpAPI key: 6c9ca2483cb494765ee7dd42c784388cd540dc50355cc39046b200ea99291784\n",
            "\n",
            "Enter keywords to analyze (comma-separated):\n",
            "> machine learning, iraq\n",
            "\n",
            "üìä Analyzing 2 keywords...\n",
            "üìã Criteria:\n",
            "   ‚Ä¢ Interest >50 at least twice in 7 days\n",
            "   ‚Ä¢ USA, Germany, Japan, UK, or France in top 10 countries\n",
            "------------------------------------------------------------\n",
            "\n",
            "[1/2] Processing: machine learning\n",
            "Analyzing: machine learning\n",
            "  Threshold check (>50 twice): True\n",
            "  Target countries check: True\n",
            "  Interest pattern: 54 ‚Üí 53 ‚Üí 52 ‚Üí 54 ‚Üí 52 ‚Üí 44 ‚Üí 47 ‚Üí 50 ‚Üí 54 ‚Üí 59 ‚Üí 74 ‚Üí 82 ‚Üí 84 ‚Üí 80 ‚Üí 86 ‚Üí 97 ‚Üí 88 ‚Üí 72 ‚Üí 72 ‚Üí 66 ‚Üí 66 ‚Üí 65 ‚Üí 58 ‚Üí 63 ‚Üí 55 ‚Üí 60 ‚Üí 49 ‚Üí 49 ‚Üí 45 ‚Üí 40 ‚Üí 39 ‚Üí 45 ‚Üí 43 ‚Üí 52 ‚Üí 57 ‚Üí 66 ‚Üí 74 ‚Üí 75 ‚Üí 75 ‚Üí 70 ‚Üí 71 ‚Üí 60 ‚Üí 61 ‚Üí 80 ‚Üí 55 ‚Üí 73 ‚Üí 77 ‚Üí 80 ‚Üí 69 ‚Üí 47 ‚Üí 41 ‚Üí 41 ‚Üí 35 ‚Üí 35 ‚Üí 33 ‚Üí 35 ‚Üí 42 ‚Üí 49 ‚Üí 60 ‚Üí 77 ‚Üí 80 ‚Üí 78 ‚Üí 86 ‚Üí 70 ‚Üí 70 ‚Üí 76 ‚Üí 100 ‚Üí 57 ‚Üí 44 ‚Üí 49 ‚Üí 46 ‚Üí 41 ‚Üí 41 ‚Üí 36 ‚Üí 37 ‚Üí 29 ‚Üí 31 ‚Üí 29 ‚Üí 29 ‚Üí 34 ‚Üí 33 ‚Üí 38 ‚Üí 46 ‚Üí 54 ‚Üí 57 ‚Üí 60 ‚Üí 58 ‚Üí 48 ‚Üí 54 ‚Üí 53 ‚Üí 50 ‚Üí 48 ‚Üí 51 ‚Üí 53 ‚Üí 52 ‚Üí 52 ‚Üí 50 ‚Üí 41 ‚Üí 37 ‚Üí 38 ‚Üí 36 ‚Üí 37 ‚Üí 37 ‚Üí 43 ‚Üí 48 ‚Üí 56 ‚Üí 61 ‚Üí 74 ‚Üí 78 ‚Üí 85 ‚Üí 78 ‚Üí 71 ‚Üí 75 ‚Üí 69 ‚Üí 61 ‚Üí 58 ‚Üí 59 ‚Üí 63 ‚Üí 63 ‚Üí 59 ‚Üí 58 ‚Üí 54 ‚Üí 48 ‚Üí 44 ‚Üí 44 ‚Üí 45 ‚Üí 41 ‚Üí 47 ‚Üí 62 ‚Üí 61 ‚Üí 75 ‚Üí 70 ‚Üí 79 ‚Üí 77 ‚Üí 79 ‚Üí 82 ‚Üí 88 ‚Üí 64 ‚Üí 59 ‚Üí 58 ‚Üí 63 ‚Üí 80 ‚Üí 66 ‚Üí 59 ‚Üí 56 ‚Üí 50 ‚Üí 49 ‚Üí 48 ‚Üí 47 ‚Üí 40 ‚Üí 40 ‚Üí 44 ‚Üí 45 ‚Üí 51 ‚Üí 64 ‚Üí 76 ‚Üí 79 ‚Üí 75 ‚Üí 81 ‚Üí 77 ‚Üí 88 ‚Üí 76 ‚Üí 63 ‚Üí 76 ‚Üí 64 ‚Üí 61 ‚Üí 59 ‚Üí 68 ‚Üí 60\n",
            "‚úÖ machine learning qualifies!\n",
            "\n",
            "[2/2] Processing: iraq\n",
            "Analyzing: iraq\n",
            "  Threshold check (>50 twice): True\n",
            "  Target countries check: False\n",
            "  Interest pattern: 39 ‚Üí 45 ‚Üí 45 ‚Üí 46 ‚Üí 48 ‚Üí 74 ‚Üí 57 ‚Üí 42 ‚Üí 39 ‚Üí 38 ‚Üí 35 ‚Üí 34 ‚Üí 36 ‚Üí 34 ‚Üí 32 ‚Üí 30 ‚Üí 29 ‚Üí 29 ‚Üí 30 ‚Üí 29 ‚Üí 28 ‚Üí 26 ‚Üí 29 ‚Üí 28 ‚Üí 28 ‚Üí 30 ‚Üí 29 ‚Üí 31 ‚Üí 30 ‚Üí 31 ‚Üí 60 ‚Üí 100 ‚Üí 84 ‚Üí 77 ‚Üí 63 ‚Üí 62 ‚Üí 57 ‚Üí 57 ‚Üí 53 ‚Üí 50 ‚Üí 54 ‚Üí 52 ‚Üí 51 ‚Üí 53 ‚Üí 50 ‚Üí 57 ‚Üí 51 ‚Üí 54 ‚Üí 66 ‚Üí 73 ‚Üí 71 ‚Üí 72 ‚Üí 72 ‚Üí 87 ‚Üí 73 ‚Üí 68 ‚Üí 65 ‚Üí 62 ‚Üí 64 ‚Üí 57 ‚Üí 60 ‚Üí 54 ‚Üí 54 ‚Üí 53 ‚Üí 50 ‚Üí 51 ‚Üí 51 ‚Üí 48 ‚Üí 56 ‚Üí 49 ‚Üí 52 ‚Üí 52 ‚Üí 53 ‚Üí 56 ‚Üí 55 ‚Üí 63 ‚Üí 58 ‚Üí 56 ‚Üí 56 ‚Üí 57 ‚Üí 52 ‚Üí 53 ‚Üí 58 ‚Üí 55 ‚Üí 50 ‚Üí 52 ‚Üí 58 ‚Üí 51 ‚Üí 50 ‚Üí 54 ‚Üí 51 ‚Üí 54 ‚Üí 52 ‚Üí 55 ‚Üí 55 ‚Üí 54 ‚Üí 53 ‚Üí 53 ‚Üí 50 ‚Üí 65 ‚Üí 60 ‚Üí 58 ‚Üí 51 ‚Üí 52 ‚Üí 54 ‚Üí 51 ‚Üí 55 ‚Üí 54 ‚Üí 48 ‚Üí 48 ‚Üí 49 ‚Üí 46 ‚Üí 47 ‚Üí 45 ‚Üí 49 ‚Üí 40 ‚Üí 42 ‚Üí 50 ‚Üí 49 ‚Üí 48 ‚Üí 50 ‚Üí 48 ‚Üí 52 ‚Üí 55 ‚Üí 57 ‚Üí 60 ‚Üí 58 ‚Üí 60 ‚Üí 62 ‚Üí 62 ‚Üí 57 ‚Üí 57 ‚Üí 53 ‚Üí 50 ‚Üí 49 ‚Üí 50 ‚Üí 52 ‚Üí 52 ‚Üí 50 ‚Üí 50 ‚Üí 65 ‚Üí 57 ‚Üí 59 ‚Üí 69 ‚Üí 67 ‚Üí 64 ‚Üí 68 ‚Üí 75 ‚Üí 73 ‚Üí 70 ‚Üí 65 ‚Üí 68 ‚Üí 70 ‚Üí 68 ‚Üí 66 ‚Üí 63 ‚Üí 57 ‚Üí 55 ‚Üí 55 ‚Üí 52 ‚Üí 52 ‚Üí 57 ‚Üí 53 ‚Üí 55 ‚Üí 56 ‚Üí 61 ‚Üí 62 ‚Üí 68 ‚Üí 75\n",
            "‚ùå iraq doesn't meet criteria\n",
            "\n",
            "üéØ FINAL RESULTS: 1 out of 2 keywords qualify\n",
            "============================================================\n",
            "\n",
            "üìà [1] MACHINE LEARNING\n",
            "   üìä Max Interest: 100%\n",
            "   üìä Avg Interest: 58%\n",
            "   üî• Days >50: 109\n",
            "   üìÖ 7-day pattern: 54 ‚Üí 53 ‚Üí 52 ‚Üí 54 ‚Üí 52 ‚Üí 44 ‚Üí 47 ‚Üí 50 ‚Üí 54 ‚Üí 59 ‚Üí 74 ‚Üí 82 ‚Üí 84 ‚Üí 80 ‚Üí 86 ‚Üí 97 ‚Üí 88 ‚Üí 72 ‚Üí 72 ‚Üí 66 ‚Üí 66 ‚Üí 65 ‚Üí 58 ‚Üí 63 ‚Üí 55 ‚Üí 60 ‚Üí 49 ‚Üí 49 ‚Üí 45 ‚Üí 40 ‚Üí 39 ‚Üí 45 ‚Üí 43 ‚Üí 52 ‚Üí 57 ‚Üí 66 ‚Üí 74 ‚Üí 75 ‚Üí 75 ‚Üí 70 ‚Üí 71 ‚Üí 60 ‚Üí 61 ‚Üí 80 ‚Üí 55 ‚Üí 73 ‚Üí 77 ‚Üí 80 ‚Üí 69 ‚Üí 47 ‚Üí 41 ‚Üí 41 ‚Üí 35 ‚Üí 35 ‚Üí 33 ‚Üí 35 ‚Üí 42 ‚Üí 49 ‚Üí 60 ‚Üí 77 ‚Üí 80 ‚Üí 78 ‚Üí 86 ‚Üí 70 ‚Üí 70 ‚Üí 76 ‚Üí 100 ‚Üí 57 ‚Üí 44 ‚Üí 49 ‚Üí 46 ‚Üí 41 ‚Üí 41 ‚Üí 36 ‚Üí 37 ‚Üí 29 ‚Üí 31 ‚Üí 29 ‚Üí 29 ‚Üí 34 ‚Üí 33 ‚Üí 38 ‚Üí 46 ‚Üí 54 ‚Üí 57 ‚Üí 60 ‚Üí 58 ‚Üí 48 ‚Üí 54 ‚Üí 53 ‚Üí 50 ‚Üí 48 ‚Üí 51 ‚Üí 53 ‚Üí 52 ‚Üí 52 ‚Üí 50 ‚Üí 41 ‚Üí 37 ‚Üí 38 ‚Üí 36 ‚Üí 37 ‚Üí 37 ‚Üí 43 ‚Üí 48 ‚Üí 56 ‚Üí 61 ‚Üí 74 ‚Üí 78 ‚Üí 85 ‚Üí 78 ‚Üí 71 ‚Üí 75 ‚Üí 69 ‚Üí 61 ‚Üí 58 ‚Üí 59 ‚Üí 63 ‚Üí 63 ‚Üí 59 ‚Üí 58 ‚Üí 54 ‚Üí 48 ‚Üí 44 ‚Üí 44 ‚Üí 45 ‚Üí 41 ‚Üí 47 ‚Üí 62 ‚Üí 61 ‚Üí 75 ‚Üí 70 ‚Üí 79 ‚Üí 77 ‚Üí 79 ‚Üí 82 ‚Üí 88 ‚Üí 64 ‚Üí 59 ‚Üí 58 ‚Üí 63 ‚Üí 80 ‚Üí 66 ‚Üí 59 ‚Üí 56 ‚Üí 50 ‚Üí 49 ‚Üí 48 ‚Üí 47 ‚Üí 40 ‚Üí 40 ‚Üí 44 ‚Üí 45 ‚Üí 51 ‚Üí 64 ‚Üí 76 ‚Üí 79 ‚Üí 75 ‚Üí 81 ‚Üí 77 ‚Üí 88 ‚Üí 76 ‚Üí 63 ‚Üí 76 ‚Üí 64 ‚Üí 61 ‚Üí 59 ‚Üí 68 ‚Üí 60\n",
            "   üåç Top countries: China, St. Helena, Ethiopia, India, Singapore\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "\n",
        "class GoogleTrendsAgent:\n",
        "    def __init__(self, serp_api_key):\n",
        "        self.api_key = serp_api_key\n",
        "        self.base_url = \"https://serpapi.com/search.json\"\n",
        "\n",
        "    def get_trends_data(self, keyword):\n",
        "        \"\"\"Get interest over time for a keyword\"\"\"\n",
        "        params = {\n",
        "            'engine': 'google_trends',\n",
        "            'q': keyword,\n",
        "            'data_type': 'TIMESERIES',\n",
        "            'date': 'now 7-d',  # Last 7 days\n",
        "            'geo': '',  # Worldwide\n",
        "            'api_key': self.api_key\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(self.base_url, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            # Check for API errors\n",
        "            if 'error' in data:\n",
        "                print(f\"API Error for {keyword}: {data['error']}\")\n",
        "                return []\n",
        "\n",
        "            # Correct path based on API documentation\n",
        "            return data.get('interest_over_time', {}).get('timeline_data', [])\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching trends for {keyword}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_region_data(self, keyword):\n",
        "        \"\"\"Get interest by region for a keyword\"\"\"\n",
        "        params = {\n",
        "            'engine': 'google_trends',\n",
        "            'q': keyword,\n",
        "            'data_type': 'GEO_MAP_0',  # Correct data_type for interest by region\n",
        "            'date': 'now 7-d',\n",
        "            'api_key': self.api_key\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(self.base_url, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            # Check for API errors\n",
        "            if 'error' in data:\n",
        "                print(f\"API Error for {keyword} regions: {data['error']}\")\n",
        "                return []\n",
        "\n",
        "            # Correct path based on API documentation\n",
        "            return data.get('interest_by_region', [])\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching regions for {keyword}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def check_threshold_criteria(self, interest_values):\n",
        "        \"\"\"Check if interest > 50 appears at least twice in 7 days\"\"\"\n",
        "        if not interest_values:\n",
        "            return False\n",
        "        days_above_50 = sum(1 for value in interest_values if value and value > 50)\n",
        "        return days_above_50 >= 2\n",
        "\n",
        "    def check_country_criteria(self, region_data):\n",
        "        \"\"\"Check if USA, Germany, Japan, UK, or France are in top countries\"\"\"\n",
        "        if not region_data:\n",
        "            return False\n",
        "\n",
        "        target_countries = ['United States', 'Germany', 'Japan', 'United Kingdom', 'France',\"China\"]\n",
        "        top_countries = [item.get('location', '') for item in region_data[:10]]\n",
        "        return any(country in top_countries for country in target_countries)\n",
        "\n",
        "    def extract_interest_values(self, trends_data, keyword):\n",
        "        \"\"\"Extract interest values from trends data for a single keyword\"\"\"\n",
        "        interest_values = []\n",
        "\n",
        "        for item in trends_data:\n",
        "            values = item.get('values', [])\n",
        "            if values:\n",
        "                # For single keyword queries, there should be one value per time period\n",
        "                if len(values) == 1:\n",
        "                    interest_values.append(values[0].get('extracted_value', 0))\n",
        "                else:\n",
        "                    # If multiple values, find the one for our keyword\n",
        "                    for value in values:\n",
        "                        if value.get('query', '').lower() == keyword.lower():\n",
        "                            interest_values.append(value.get('extracted_value', 0))\n",
        "                            break\n",
        "                    else:\n",
        "                        # If keyword not found, append 0\n",
        "                        interest_values.append(0)\n",
        "\n",
        "        return interest_values\n",
        "\n",
        "    def analyze_keyword(self, keyword):\n",
        "        \"\"\"Analyze a single keyword\"\"\"\n",
        "        print(f\"Analyzing: {keyword}\")\n",
        "\n",
        "        # Get trends data\n",
        "        trends_data = self.get_trends_data(keyword)\n",
        "        if not trends_data:\n",
        "            print(f\"No trends data for {keyword}\")\n",
        "            return None\n",
        "\n",
        "        # Extract interest values\n",
        "        interest_values = self.extract_interest_values(trends_data, keyword)\n",
        "\n",
        "        if len(interest_values) < 7:\n",
        "            print(f\"Not enough data for {keyword} (got {len(interest_values)} data points)\")\n",
        "            return None\n",
        "\n",
        "        # Get region data with rate limiting\n",
        "        time.sleep(1)  # Rate limiting between API calls\n",
        "        region_data = self.get_region_data(keyword)\n",
        "\n",
        "        # Apply your logic\n",
        "        meets_threshold = self.check_threshold_criteria(interest_values)\n",
        "        has_target_countries = self.check_country_criteria(region_data)\n",
        "\n",
        "        print(f\"  Threshold check (>50 twice): {meets_threshold}\")\n",
        "        print(f\"  Target countries check: {has_target_countries}\")\n",
        "        print(f\"  Interest pattern: {' ‚Üí '.join(map(str, interest_values))}\")\n",
        "\n",
        "        if meets_threshold and has_target_countries:\n",
        "            return {\n",
        "                'keyword': keyword,\n",
        "                'interest_values': interest_values,\n",
        "                'max_interest': max(interest_values) if interest_values else 0,\n",
        "                'avg_interest': round(sum(interest_values) / len(interest_values)) if interest_values else 0,\n",
        "                'days_above_50': sum(1 for v in interest_values if v and v > 50),\n",
        "                'top_countries': [item.get('location', '') for item in region_data[:5]]\n",
        "            }\n",
        "\n",
        "        return None\n",
        "\n",
        "    def process_keywords(self, keywords):\n",
        "        \"\"\"Process list of keywords and return only those meeting criteria\"\"\"\n",
        "        qualifying_keywords = []\n",
        "\n",
        "        for i, keyword in enumerate(keywords):\n",
        "            keyword = keyword.strip()\n",
        "            if not keyword:\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n[{i+1}/{len(keywords)}] Processing: {keyword}\")\n",
        "\n",
        "            result = self.analyze_keyword(keyword)\n",
        "            if result:\n",
        "                qualifying_keywords.append(result)\n",
        "                print(f\"‚úÖ {keyword} qualifies!\")\n",
        "            else:\n",
        "                print(f\"‚ùå {keyword} doesn't meet criteria\")\n",
        "\n",
        "            # Rate limiting between keywords\n",
        "            if i < len(keywords) - 1:  # Don't sleep after the last keyword\n",
        "                time.sleep(2)\n",
        "\n",
        "        return qualifying_keywords\n",
        "\n",
        "def main():\n",
        "    print(\"üîç Google Trends Keyword Analyzer\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Your SERP API key\n",
        "    API_KEY = input(\"Enter your SerpAPI key: \").strip()\n",
        "\n",
        "    if not API_KEY:\n",
        "        print(\"‚ùå API key is required!\")\n",
        "        return\n",
        "\n",
        "    # Initialize agent\n",
        "    agent = GoogleTrendsAgent(API_KEY)\n",
        "\n",
        "    # Input keywords\n",
        "    print(\"\\nEnter keywords to analyze (comma-separated):\")\n",
        "    keywords_input = input(\"> \").strip()\n",
        "\n",
        "    if not keywords_input:\n",
        "        print(\"‚ùå No keywords provided!\")\n",
        "        return\n",
        "\n",
        "    keywords = [k.strip() for k in keywords_input.split(',') if k.strip()]\n",
        "\n",
        "    print(f\"\\nüìä Analyzing {len(keywords)} keywords...\")\n",
        "    print(\"üìã Criteria:\")\n",
        "    print(\"   ‚Ä¢ Interest >50 at least twice in 7 days\")\n",
        "    print(\"   ‚Ä¢ USA, Germany, Japan, UK, or France in top 10 countries\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Process keywords\n",
        "    results = agent.process_keywords(keywords)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\nüéØ FINAL RESULTS: {len(results)} out of {len(keywords)} keywords qualify\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if results:\n",
        "        for i, result in enumerate(results, 1):\n",
        "            print(f\"\\nüìà [{i}] {result['keyword'].upper()}\")\n",
        "            print(f\"   üìä Max Interest: {result['max_interest']}%\")\n",
        "            print(f\"   üìä Avg Interest: {result['avg_interest']}%\")\n",
        "            print(f\"   üî• Days >50: {result['days_above_50']}\")\n",
        "            print(f\"   üìÖ 7-day pattern: {' ‚Üí '.join(map(str, result['interest_values']))}\")\n",
        "            print(f\"   üåç Top countries: {', '.join(result['top_countries'])}\")\n",
        "    else:\n",
        "        print(\"‚ùå No keywords met the criteria.\")\n",
        "        print(\"\\nüí° Tips:\")\n",
        "        print(\"   ‚Ä¢ Try more popular/trending keywords\")\n",
        "        print(\"   ‚Ä¢ Check if keywords have sufficient search volume\")\n",
        "        print(\"   ‚Ä¢ Verify your API key has sufficient credits\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AylUzRjJL2Vg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}